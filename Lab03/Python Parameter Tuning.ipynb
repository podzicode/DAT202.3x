{"nbformat_minor": 1, "cells": [{"source": "## Tuning Model Parameters\n\nIn this exercise, you will optimise the parameters for a classification model.\n\n### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Load the source data\ncsv = spark.read.csv('wasb:///data/flights.csv', inferSchema=True, header=True)\n\n# Select features and label\ndata = csv.select(\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"label\")))\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a classification model", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Define the pipeline\nassembler = VectorAssembler(inputCols = [\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\npipeline = Pipeline(stages=[assembler, lr])", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Tune Parameters\nYou can tune parameters to find the best model for your data. A simple way to do this is to use  **TrainValidationSplit** to evaluate each combination of parameters defined in a **ParameterGrid** against a subset of the training data in order to find the best performing parameters.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.3, 0.1, 0.01]).addGrid(lr.maxIter, [10, 5]).addGrid(lr.threshold, [0.35, 0.30]).build()\ntvs = TrainValidationSplit(estimator=pipeline, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\n\nmodel = tvs.fit(train)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Test the Model\nNow you're ready to apply the model to the test data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "prediction = model.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\npredicted.show(100)", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naur = evaluator.evaluate(prediction)\nprint \"AUR = \", aur", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}