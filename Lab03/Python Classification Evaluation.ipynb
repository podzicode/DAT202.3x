{"nbformat_minor": 1, "cells": [{"source": "## Evaluating a Classification Model\n\nIn this exercise, you will create a pipeline for a classification model, and then apply commonly used metrics to evaluate the resulting classifier.\n\n### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Load the source data\ncsv = spark.read.csv('wasb:///data/flights.csv', inferSchema=True, header=True)\n\n# Select features and label\ndata = csv.select(\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"label\")))\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Define the Pipeline and Train the Model\nNow define a pipeline that creates a feature vector and trains a classification model", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "assembler = VectorAssembler(inputCols = [\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\npipeline = Pipeline(stages=[assembler, lr])\nmodel = pipeline.fit(train)", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Test the Model\nNow you're ready to apply the model to the test data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "prediction = model.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(100, truncate=False)", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### View the Raw Prediction and Probability\nThe prediction is based on a raw prediction score that describes a labelled point in a logistic function. This raw prediction is then converted to a predicted label of 0 or 1 based on a probability vector that indicates the confidence for each possible label value (in this case, 0 and 1). The value with the highest confidence is selected as the prediction.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "prediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(100, truncate=False)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Note that the results include rows where the probability for 0 (the first value in the **probability** vector) is only slightly higher than the probability for 1 (the second value in the **probability** vector). The default *discrimination threshold* (the boundary that decides whether a probability is predicted as a 1 or a 0) is set to 0.5; so the prediction with the highest probability is always used, no matter how close to the threshold.", "cell_type": "markdown", "metadata": {}}, {"source": "### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this. The ROC curve shows the True Positive and False Positive rates plotted for varying thresholds.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\naur = evaluator.evaluate(prediction)\nprint \"AUR = \", aur", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Change the Discrimination Threshold\nThe AUC score seems to indicate a reasonably good model, but the performance metrics seem to indicate that it predicts a high number of False Negative labels (i.e. it predicts 0 when the true label is 1), leading to a low Recall. You can affect the way a model performs by changing its parameters. For example, as noted previously, the default discrimination threshold is set to 0.5 - so if there are a lot of False Positives, you may want to consider raising this; or conversely, you may want to address a large number of False Negatives by lowering the threshold.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "lr2 = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3, threshold=0.35)\npipeline2 = Pipeline(stages=[assembler, lr2])\nmodel2 = pipeline2.fit(train)\nnewPrediction = model2.transform(test)\nnewPrediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(100, truncate=False)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Note that some of the **rawPrediction** and **probability** values that were previously predicted as 0 are now predicted as 1", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Recalculate confusion matrix\ntp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics2 = spark.createDataFrame([\n (\"TP\", tp2),\n (\"FP\", fp2),\n (\"TN\", tn2),\n (\"FN\", fn2),\n (\"Precision\", tp2 / (tp2 + fp2)),\n (\"Recall\", tp2 / (tp2 + fn2))],[\"metric\", \"value\"])\nmetrics2.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Note that there are now more True Positives and less False Negatives, and Recall has improved. By changing the discrimination threshold, the model now gets more predictions correct - though it's worth noting that the number of False Positives has also increased.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}