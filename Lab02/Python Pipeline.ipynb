{"nbformat_minor": 1, "cells": [{"source": "## Creating a Pipeline\n\nIn this exercise, you will implement a pipeline that includes multiple stages of *transformers* and *estimators* to prepare features and train a classification model. The resulting trained *PipelineModel* can then be used as a transformer to predict whether or not a flight will be late.\n\n### Import Spark SQL and Spark ML Libraries\n\nFirst, import the libraries you will need:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Load Source Data\nThe data for this exercise is provided as a CSV file containing details of flights. The data includes specific characteristics (or *features*) for each flight, as well as a column indicating how many minutes late or early the flight arrived.\n\nYou will load this data into a DataFrame and display it.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "csv = spark.read.csv('wasb:///data/flights.csv', inferSchema=True, header=True)\ncsv.show()", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Prepare the Data\nMost modeling begins with exhaustive exploration and preparation of the data. In this example, the data has been cleaned for you. You will simply select a subset of columns to use as *features* and create a Boolean *label* field named **label** with the value **1** for flights that arrived 15 minutes or more after the scheduled arrival time, or **0** if the flight was early or on-time.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data = csv.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Double\").alias(\"label\")))\ndata.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Split the Data\nIt is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 70% of the data for training, and reserve 30% for testing. In the testing data, the **label** column is renamed to **trueLabel** so you can use it later to compare predicted labels with known actual values.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "splits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\ntrain_rows = train.count()\ntest_rows = test.count()\nprint \"Training Rows:\", train_rows, \" Testing Rows:\", test_rows", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Define the Pipeline\nA predictive model often requires multiple stages of feature preparation. For example, it is common when using some algorithms to distingish between continuous features (which have a calculable numeric value) and categorical features (which are numeric representations of discrete categories). It is also common to *normalize* continuous numeric features to use a common scale (for example, by scaling all numbers to a proportinal decimal value between 0 and 1).\n\nA pipeline consists of a a series of *transformer* and *estimator* stages that typically prepare a DataFrame for\nmodeling and then train a predictive model. In this case, you will create a pipeline with seven stages:\n- A **StringIndexer** estimator that converts string values to indexes for categorical features\n- A **VectorAssembler** that combines categorical features into a single vector\n- A **VectorIndexer** that creates indexes for a vector of categorical features\n- A **VectorAssembler** that creates a vector of continuous numeric features\n- A **MinMaxScaler** that normalizes continuous numeric features\n- A **VectorAssembler** that creates a vector of categorical and continuous features\n- A **DecisionTreeClassifier** that trains a classification model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "strIdx = StringIndexer(inputCol = \"Carrier\", outputCol = \"CarrierIdx\")\ncatVect = VectorAssembler(inputCols = [\"CarrierIdx\", \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\"], outputCol=\"catFeatures\")\ncatIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\nnumVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\nminMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\nfeatVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\npipeline = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, dt])", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Run the Pipeline as an Estimator\nThe pipeline itself is an estimator, and so it has a **fit** method that you can call to run the pipeline on a specified DataFrame. In this case, you will run the pipeline on the training data to train a model. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "piplineModel = pipeline.fit(train)\nprint \"Pipeline complete!\"", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "### Test the Pipeline Model\nThe model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. In this case, you will transform the **test** DataFrame using the pipeline to generate label predictions.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "prediction = piplineModel.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(100, truncate=False)", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "The resulting DataFrame is produced by applying all of the transformations in the pipline to the test data. The **prediction** column contains the predicted value for the label, and the **trueLabel** column contains the actual known value from the testing data.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}